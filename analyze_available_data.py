#!/usr/bin/env python3
"""
Comprehensive Data Analysis for Claude Code Optimizer
====================================================
Analyzes all available data sources to determine what can be displayed on dashboard
"""

import json
import sqlite3
import glob
import os
from datetime import datetime
from collections import defaultdict, Counter

def analyze_jsonl_files():
    """Analyze JSONL files to see all available data"""
    print("üìÑ CLAUDE CODE JSONL FILES ANALYSIS:")
    print("=" * 50)
    
    # Find all recent JSONL files
    patterns = [
        "/Users/jordaaan/Library/Mobile Documents/com~apple~CloudDocs/claude-backup/projects/**/*.jsonl",
        "/Users/jordaaan/.claude/projects/**/*.jsonl"
    ]
    
    jsonl_files = []
    for pattern in patterns:
        files = glob.glob(pattern, recursive=True)
        for file in files:
            if os.path.exists(file) and os.path.getmtime(file) > (datetime.now().timestamp() - 86400):
                jsonl_files.append(file)
    
    if not jsonl_files:
        print("‚ùå No recent JSONL files found")
        return {}
    
    # Analyze the most recent file
    latest_file = max(jsonl_files, key=lambda x: os.path.getmtime(x))
    print(f"üìÅ Analyzing: {os.path.basename(latest_file)}")
    
    all_fields = set()
    usage_fields = set()
    message_fields = set()
    models_seen = set()
    conversation_data = []
    token_breakdown = defaultdict(int)
    
    try:
        with open(latest_file, 'r') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    
                    # Collect all fields
                    all_fields.update(data.keys())
                    
                    # Analyze messages
                    if 'message' in data:
                        message = data['message']
                        message_fields.update(message.keys())
                        
                        # Model information
                        if 'model' in message:
                            models_seen.add(message['model'])
                        
                        # Usage analysis
                        if 'usage' in message:
                            usage = message['usage']
                            usage_fields.update(usage.keys())
                            
                            # Aggregate token data
                            for token_type, count in usage.items():
                                if isinstance(count, (int, float)):
                                    token_breakdown[token_type] += count
                    
                    # Store conversation flow data
                    conversation_data.append({
                        'type': data.get('type'),
                        'timestamp': data.get('timestamp'),
                        'session_id': data.get('sessionId'),
                        'version': data.get('version'),
                        'cwd': data.get('cwd'),
                        'git_branch': data.get('gitBranch'),
                        'request_id': data.get('requestId'),
                        'parent_uuid': data.get('parentUuid'),
                        'user_type': data.get('userType')
                    })
                    
                except json.JSONDecodeError:
                    continue
    except Exception as e:
        print(f"‚ùå Error reading JSONL: {e}")
        return {}
    
    # Print analysis results
    print(f"\nüîç JSONL FIELD ANALYSIS:")
    print(f"   üìä Total Records: {len(conversation_data)}")
    print(f"   ü§ñ Models Used: {', '.join(models_seen)}")
    print(f"   üìÖ Date Range: Available in timestamps")
    
    print(f"\nüìã ALL AVAILABLE TOP-LEVEL FIELDS:")
    for field in sorted(all_fields):
        print(f"   ‚Ä¢ {field}")
    
    print(f"\nüí¨ MESSAGE STRUCTURE FIELDS:")
    for field in sorted(message_fields):
        print(f"   ‚Ä¢ message.{field}")
    
    print(f"\nüí∞ TOKEN USAGE BREAKDOWN:")
    for field in sorted(usage_fields):
        total = token_breakdown.get(field, 0)
        print(f"   ‚Ä¢ usage.{field}: {total:,} total")
    
    return {
        'all_fields': list(all_fields),
        'message_fields': list(message_fields),
        'usage_fields': list(usage_fields),
        'models': list(models_seen),
        'conversation_count': len(conversation_data),
        'token_breakdown': dict(token_breakdown),
        'conversation_data': conversation_data[:5]  # Sample data
    }

def analyze_database():
    """Analyze database structure and available data"""
    print(f"\nüóÑÔ∏è DATABASE STRUCTURE ANALYSIS:")
    print("=" * 50)
    
    try:
        conn = sqlite3.connect('claude_usage.db')
        cursor = conn.cursor()
        
        # Get all tables
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = cursor.fetchall()
        
        db_analysis = {}
        
        for table_name, in tables:
            print(f"\nüìä TABLE: {table_name}")
            
            # Get table schema
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()
            
            print("   Columns:")
            for col in columns:
                col_name, col_type, not_null, default, pk = col[1], col[2], col[3], col[4], col[5]
                print(f"     ‚Ä¢ {col_name} ({col_type})")
            
            # Get sample data
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            print(f"   Records: {count}")
            
            if count > 0:
                # Get sample record
                cursor.execute(f"SELECT * FROM {table_name} LIMIT 1")
                sample = cursor.fetchone()
                if sample:
                    print("   Sample data available ‚úÖ")
            
            db_analysis[table_name] = {
                'columns': [col[1] for col in columns],
                'count': count
            }
        
        conn.close()
        return db_analysis
        
    except Exception as e:
        print(f"‚ùå Error analyzing database: {e}")
        return {}

def analyze_calculated_metrics():
    """Identify what metrics can be calculated from available data"""
    print(f"\nüìà CALCULABLE METRICS & ANALYTICS:")
    print("=" * 50)
    
    metrics = {
        "üìä Session Analytics": [
            "‚Ä¢ Sessions per day/week/month",
            "‚Ä¢ Average session duration",
            "‚Ä¢ Session frequency patterns",
            "‚Ä¢ Active hours heatmap",
            "‚Ä¢ Session efficiency scoring",
            "‚Ä¢ Conversation chain length analysis"
        ],
        
        "üí∞ Token Analytics": [
            "‚Ä¢ Input vs Output token ratios",
            "‚Ä¢ Cache hit rates (cache_read vs cache_creation)",
            "‚Ä¢ Token efficiency per prompt",
            "‚Ä¢ Cost per session/day/week",
            "‚Ä¢ Token usage trends over time",
            "‚Ä¢ Model-specific token consumption"
        ],
        
        "ü§ñ Model Usage Analytics": [
            "‚Ä¢ Sonnet vs Opus usage breakdown",
            "‚Ä¢ Model switching patterns",
            "‚Ä¢ Model performance comparison",
            "‚Ä¢ Cost optimization opportunities",
            "‚Ä¢ Model recommendation accuracy"
        ],
        
        "üìÖ Time-based Analytics": [
            "‚Ä¢ Peak usage hours",
            "‚Ä¢ Day-of-week patterns",
            "‚Ä¢ Session duration distributions",
            "‚Ä¢ Rate limit approach warnings",
            "‚Ä¢ Time-to-limit projections"
        ],
        
        "üíª Project/Context Analytics": [
            "‚Ä¢ Most active projects (by CWD)",
            "‚Ä¢ Git branch correlation",
            "‚Ä¢ Claude Code version tracking",
            "‚Ä¢ Project complexity correlation",
            "‚Ä¢ Context switching patterns"
        ],
        
        "‚ö° Performance Analytics": [
            "‚Ä¢ Response time analysis",
            "‚Ä¢ Request success rates",
            "‚Ä¢ Cache performance metrics",
            "‚Ä¢ Session reliability scoring",
            "‚Ä¢ Error rate tracking"
        ],
        
        "üéØ Rate Limit Analytics": [
            "‚Ä¢ 5-hour block utilization",
            "‚Ä¢ Weekly usage prediction",
            "‚Ä¢ Limit approach alerts",
            "‚Ä¢ Usage optimization suggestions",
            "‚Ä¢ Budget burn rate analysis"
        ]
    }
    
    for category, items in metrics.items():
        print(f"\n{category}")
        for item in items:
            print(f"  {item}")
    
    return metrics

def analyze_monitoring_data():
    """Check what monitoring/tracking data is available"""
    print(f"\nüîÑ MONITORING & TRACKING DATA:")
    print("=" * 50)
    
    monitoring_files = [
        "moonlock-dashboard/real-session-data.json",
        "moonlock-dashboard/rate-limit-status.json",
        "session_tracker/dashboard.log",
        "session_tracker/enhanced_monitor_v3.log"
    ]
    
    available_data = {}
    
    for file_path in monitoring_files:
        if os.path.exists(file_path):
            try:
                file_size = os.path.getsize(file_path)
                mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                print(f"‚úÖ {file_path}")
                print(f"   Size: {file_size:,} bytes")
                print(f"   Modified: {mod_time}")
                
                # Try to parse JSON files
                if file_path.endswith('.json'):
                    with open(file_path, 'r') as f:
                        data = json.load(f)
                        print(f"   Fields: {list(data.keys())}")
                        available_data[file_path] = data
                
            except Exception as e:
                print(f"‚ö†Ô∏è {file_path}: Error reading ({e})")
        else:
            print(f"‚ùå {file_path}: Not found")
    
    return available_data

def generate_dashboard_recommendations():
    """Generate recommendations for dashboard enhancements"""
    print(f"\nüé® DASHBOARD ENHANCEMENT RECOMMENDATIONS:")
    print("=" * 50)
    
    recommendations = {
        "üî• High Impact Additions": [
            "üìä Real-time token consumption rate (tokens/minute)",
            "‚è∞ Session duration with progress bar",
            "üéØ Efficiency score (output quality vs tokens used)",
            "üìà Usage trend chart (last 7 days)",
            "‚ö° Cache hit rate percentage",
            "üí° Cost optimization suggestions"
        ],
        
        "üìä Advanced Analytics": [
            "üìÖ Weekly usage heatmap",
            "ü§ñ Model performance comparison chart",
            "üìà Token efficiency trends",
            "‚è±Ô∏è Response time analytics", 
            "üîÑ Session chaining analysis",
            "üìä Project-based usage breakdown"
        ],
        
        "üö® Alert Systems": [
            "‚ö†Ô∏è Rate limit proximity warnings",
            "üìä Unusual usage pattern detection",
            "üí∞ Budget burn rate alerts",
            "‚è∞ Session duration warnings",
            "üéØ Efficiency drop notifications"
        ],
        
        "üéÆ Interactive Features": [
            "üîç Session deep-dive viewer",
            "üìä Custom date range filtering",
            "üìà Metric comparison tools",
            "‚öôÔ∏è Threshold configuration",
            "üì§ Usage report export"
        ]
    }
    
    for category, items in recommendations.items():
        print(f"\n{category}")
        for item in items:
            print(f"  {item}")

if __name__ == "__main__":
    print("üîç COMPREHENSIVE DATA ANALYSIS FOR DASHBOARD")
    print("=" * 60)
    
    jsonl_data = analyze_jsonl_files()
    db_data = analyze_database()
    metrics = analyze_calculated_metrics()
    monitoring_data = analyze_monitoring_data()
    generate_dashboard_recommendations()
    
    print(f"\n‚úÖ ANALYSIS COMPLETE!")
    print("=" * 60)
    print("üìä Data sources analyzed:")
    print(f"   ‚Ä¢ JSONL files: {len(jsonl_data)} field types")
    print(f"   ‚Ä¢ Database tables: {len(db_data)}")
    print(f"   ‚Ä¢ Monitoring files: {len(monitoring_data)}")
    print(f"   ‚Ä¢ Calculable metrics: {len(metrics)} categories")
